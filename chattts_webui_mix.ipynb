{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> 🌟 如果你觉得 ChatTTS 和 ChatTTS_colab 项目对你有帮助，请访问以下链接给它们点个星星吧！🌟\n",
        "\n",
        "- [ChatTTS 项目](https://github.com/2noise/ChatTTS)\n",
        "\n",
        "- [ChatTTS_colab 项目](https://github.com/6drf21e/ChatTTS_colab)\n",
        "\n",
        "感谢你的支持！\n",
        "\n",
        "# 运行方法\n",
        "\n",
        "- 点击菜单栏的--代码执行程序--全部运行即可\n",
        "- 执行后在下方的日志中找到类似\n",
        "\n",
        "  Running on public URL: https://**************.gradio.live  <-这个就是可以访问的公网地址\n",
        "\n",
        "安装包的时候提示要重启 请点**\"否\"**"
      ],
      "metadata": {
        "id": "Xo3k5XsTzWK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -q https://github.com/6drf21e/ChatTTS_colab\n",
        "%cd ChatTTS_colab\n",
        "!git clone -q https://github.com/2noise/ChatTTS\n",
        "%cd ChatTTS\n",
        "!git checkout -q e6412b1\n",
        "%cd ..\n",
        "!mv ChatTTS abc\n",
        "!mv abc/* /content/ChatTTS_colab/\n",
        "!pip install -q omegaconf vocos vector_quantize_pytorch gradio cn2an pypinyin openai jieba WeTextProcessing python-dotenv\n",
        "# 启动 Gradio 有公网地址\n",
        "!python webui_mix.py --share\n"
      ],
      "metadata": {
        "id": "hNDl-5muR77-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa5cffe-0e10-4a9b-d9db-92adc1cf80c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ChatTTS_colab\n",
            "/content/ChatTTS_colab/ChatTTS\n",
            "/content/ChatTTS_colab\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "2025-03-10 12:00:20.284509: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741608020.526782    1501 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741608020.593266    1501 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-10 12:00:21.117203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading ChatTTS model...\n",
            "INFO:ChatTTS.core:Download from HF: https://huggingface.co/2Noise/ChatTTS\n",
            "Fetching 12 files:   0% 0/12 [00:00<?, ?it/s]\n",
            "spk_stat.pt: 100% 4.26k/4.26k [00:00<00:00, 12.5MB/s]\n",
            "\n",
            "DVAE.pt:   0% 0.00/27.7M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Decoder.pt:   0% 0.00/104M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Vocos.pt:   0% 0.00/54.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   0% 0.00/901M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DVAE_full.pt:   0% 0.00/60.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder.yaml: 100% 117/117 [00:00<00:00, 624kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.pt: 100% 337k/337k [00:00<00:00, 5.60MB/s]\n",
            "\n",
            "DVAE.pt:  38% 10.5M/27.7M [00:00<00:00, 70.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DVAE_full.pt:  17% 10.5M/60.4M [00:00<00:00, 77.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Decoder.pt:  10% 10.5M/104M [00:00<00:01, 62.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   1% 10.5M/901M [00:00<00:15, 58.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "dvae.yaml: 100% 143/143 [00:00<00:00, 823kB/s]\n",
            "\n",
            "\n",
            "\n",
            "Vocos.pt:  19% 10.5M/54.4M [00:00<00:00, 51.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gpt.yaml: 100% 346/346 [00:00<00:00, 2.13MB/s]\n",
            "\n",
            "DVAE.pt:  76% 21.0M/27.7M [00:00<00:00, 69.7MB/s]\u001b[A\n",
            "\n",
            "Decoder.pt:  20% 21.0M/104M [00:00<00:01, 67.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   2% 21.0M/901M [00:00<00:12, 72.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "path.yaml: 100% 309/309 [00:00<00:00, 1.32MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DVAE_full.pt:  35% 21.0M/60.4M [00:00<00:00, 62.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "DVAE.pt: 100% 27.7M/27.7M [00:00<00:00, 60.2MB/s]\n",
            "\n",
            "\n",
            "Fetching 12 files:   8% 1/12 [00:00<00:06,  1.59it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DVAE_full.pt:  52% 31.5M/60.4M [00:00<00:00, 73.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Vocos.pt:  58% 31.5M/54.4M [00:00<00:00, 60.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "vocos.yaml: 100% 460/460 [00:00<00:00, 2.07MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   5% 41.9M/901M [00:00<00:10, 78.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Decoder.pt:  40% 41.9M/104M [00:00<00:00, 74.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DVAE_full.pt:  69% 41.9M/60.4M [00:00<00:00, 69.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Vocos.pt:  77% 41.9M/54.4M [00:00<00:00, 68.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   6% 52.4M/901M [00:00<00:10, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Vocos.pt:  96% 52.4M/54.4M [00:00<00:00, 76.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Decoder.pt:  61% 62.9M/104M [00:00<00:00, 80.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   7% 62.9M/901M [00:00<00:10, 80.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Vocos.pt: 100% 54.4M/54.4M [00:00<00:00, 62.3MB/s]\n",
            "DVAE_full.pt: 100% 60.4M/60.4M [00:00<00:00, 68.8MB/s]\n",
            "Fetching 12 files:  17% 2/12 [00:01<00:05,  1.93it/s]\n",
            "\n",
            "Decoder.pt:  71% 73.4M/104M [00:00<00:00, 81.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   9% 83.9M/901M [00:00<00:08, 96.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Decoder.pt:  91% 94.4M/104M [00:01<00:00, 99.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Decoder.pt: 100% 104M/104M [00:01<00:00, 88.7MB/s] \n",
            "Fetching 12 files:  25% 3/12 [00:01<00:03,  2.50it/s]\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  14% 126M/901M [00:01<00:05, 129MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  17% 157M/901M [00:01<00:04, 159MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  21% 189M/901M [00:01<00:03, 182MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  23% 210M/901M [00:01<00:03, 188MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  27% 241M/901M [00:01<00:03, 199MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  29% 262M/901M [00:01<00:03, 200MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  33% 294M/901M [00:01<00:02, 211MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  36% 325M/901M [00:02<00:02, 213MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  40% 357M/901M [00:02<00:02, 218MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  43% 388M/901M [00:02<00:02, 215MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  47% 419M/901M [00:02<00:02, 220MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  50% 451M/901M [00:02<00:01, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  54% 482M/901M [00:02<00:01, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  57% 514M/901M [00:02<00:01, 236MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  61% 545M/901M [00:03<00:01, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  64% 577M/901M [00:03<00:01, 234MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  68% 608M/901M [00:03<00:01, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  71% 640M/901M [00:03<00:01, 224MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  75% 671M/901M [00:03<00:01, 227MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  78% 703M/901M [00:03<00:00, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  81% 734M/901M [00:03<00:00, 228MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  85% 765M/901M [00:04<00:00, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  88% 797M/901M [00:04<00:00, 221MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  92% 828M/901M [00:04<00:00, 229MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  95% 860M/901M [00:04<00:00, 240MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt: 100% 901M/901M [00:04<00:00, 195MB/s]\n",
            "Fetching 12 files: 100% 12/12 [00:04<00:00,  2.49it/s]\n",
            "INFO:ChatTTS.core:use cuda:0\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vocos.load_state_dict(torch.load(vocos_ckpt_path))\n",
            "INFO:ChatTTS.core:vocos loaded.\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dvae.load_state_dict(torch.load(dvae_ckpt_path))\n",
            "INFO:ChatTTS.core:dvae loaded.\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  gpt.load_state_dict(torch.load(gpt_ckpt_path))\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.pretrain_models['spk_stat'] = torch.load(spk_stat_path).to(device)\n",
            "INFO:ChatTTS.core:gpt loaded.\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder.load_state_dict(torch.load(decoder_ckpt_path, map_location='cpu'))\n",
            "INFO:ChatTTS.core:decoder loaded.\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:144: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  tokenizer = torch.load(tokenizer_path, map_location='cpu')\n",
            "INFO:ChatTTS.core:tokenizer loaded.\n",
            "INFO:ChatTTS.core:All initialized.\n",
            "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64 \"HTTP/1.1 200 OK\"\n",
            "* Running on public URL: https://76523d8b6e76434098.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "[['旁白', 0], ['年轻女性', 0], ['中年男性', 0]]\n",
            "[{'character': '旁白', 'seed': 2222, 'speed': 3, 'oral': 0, 'laugh': 0, 'break': 2}, {'character': '年轻女性', 'seed': 2, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 2}, {'character': '中年男性', 'seed': 2424, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 2}]\n",
            "{'旁白': {'character': '旁白', 'seed': 2222, 'speed': 3, 'oral': 0, 'laugh': 0, 'break': 2}, '年轻女性': {'character': '年轻女性', 'seed': 2, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 2}, '中年男性': {'character': '中年男性', 'seed': 2424, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 2}}\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.647 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.647 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n",
            "seed=2222 t=['在一个风和日丽的下午，小红帽准备去森林里看望她的奶奶。', '小红帽说', '在森林里，小红帽遇到了狡猾的大灰狼。', '大灰狼说', '小红帽回答'] c=旁白 s=3 r=[oral_0][laugh_0][break_2]\n",
            "speaker_type: seed\n",
            "Inferring audio for seed=2222:   0% 0/2 [00:00<?, ?steps/s]INFO:ChatTTS.core:All initialized.\n",
            "2025-03-10 12:06:14,282 WETEXT INFO found existing fst: /usr/local/lib/python3.11/dist-packages/tn/zh_tn_tagger.fst\n",
            "INFO:wetext-zh_normalizer:found existing fst: /usr/local/lib/python3.11/dist-packages/tn/zh_tn_tagger.fst\n",
            "2025-03-10 12:06:14,282 WETEXT INFO                     /usr/local/lib/python3.11/dist-packages/tn/zh_tn_verbalizer.fst\n",
            "INFO:wetext-zh_normalizer:                    /usr/local/lib/python3.11/dist-packages/tn/zh_tn_verbalizer.fst\n",
            "2025-03-10 12:06:14,283 WETEXT INFO skip building fst for zh_normalizer ...\n",
            "INFO:wetext-zh_normalizer:skip building fst for zh_normalizer ...\n",
            "INFO:ChatTTS.core:homophones_replacer loaded.\n",
            "Inferring audio for seed=2222:   0% 0/2 [00:00<?, ?steps/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2103, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1650, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 870, in generate_script_audio\n",
            "    wavs = generate_audio_for_seed(chat, int(seed), texts, DEFAULT_BATCH_SIZE, speed,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/tts_model.py\", line 110, in generate_audio_for_seed\n",
            "    wavs = chat.infer(batch, params_infer_code=_params_infer_code, params_refine_text=params_refine_text,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 259, in infer\n",
            "    return next(res_gen)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 187, in _infer\n",
            "    text_tokens = refine_text(\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/infer/api.py\", line 97, in refine_text\n",
            "    text_token = models['tokenizer'](text, return_tensors='pt', add_special_tokens=False, padding=True).to(device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2868, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2956, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3149, in batch_encode_plus\n",
            "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
            "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2769, in _get_padding_truncation_strategies\n",
            "    if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id < 0):\n",
            "                                                           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n",
            "    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\n",
            "AttributeError: BertTokenizerFast has no attribute pad_token\n",
            "[['旁白', 0], ['年轻女性', 0], ['中年男性', 0]]\n",
            "[{'character': '旁白', 'seed': 2222, 'speed': 3, 'oral': 0, 'laugh': 0, 'break': 2}, {'character': '年轻女性', 'seed': 2, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 2}, {'character': '中年男性', 'seed': 2424, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 2}]\n",
            "{'旁白': {'character': '旁白', 'seed': 2222, 'speed': 3, 'oral': 0, 'laugh': 0, 'break': 2}, '年轻女性': {'character': '年轻女性', 'seed': 2, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 2}, '中年男性': {'character': '中年男性', 'seed': 2424, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 2}}\n",
            "seed=2222 t=['在一个风和日丽的下午，小红帽准备去森林里看望她的奶奶。', '小红帽说', '在森林里，小红帽遇到了狡猾的大灰狼。', '大灰狼说', '小红帽回答'] c=旁白 s=3 r=[oral_0][laugh_0][break_2]\n",
            "speaker_type: seed\n",
            "Inferring audio for seed=2222:   0% 0/2 [00:00<?, ?steps/s]INFO:ChatTTS.core:All initialized.\n",
            "Inferring audio for seed=2222:   0% 0/2 [00:00<?, ?steps/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2103, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1650, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 870, in generate_script_audio\n",
            "    wavs = generate_audio_for_seed(chat, int(seed), texts, DEFAULT_BATCH_SIZE, speed,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/tts_model.py\", line 110, in generate_audio_for_seed\n",
            "    wavs = chat.infer(batch, params_infer_code=_params_infer_code, params_refine_text=params_refine_text,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 259, in infer\n",
            "    return next(res_gen)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 187, in _infer\n",
            "    text_tokens = refine_text(\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/infer/api.py\", line 97, in refine_text\n",
            "    text_token = models['tokenizer'](text, return_tensors='pt', add_special_tokens=False, padding=True).to(device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2868, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2956, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3149, in batch_encode_plus\n",
            "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
            "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2769, in _get_padding_truncation_strategies\n",
            "    if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id < 0):\n",
            "                                                           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n",
            "    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\n",
            "AttributeError: BertTokenizerFast has no attribute pad_token\n",
            "[['旁白', 0], ['年轻女性', 0], ['中年男性', 0]]\n",
            "[['旁白', 0], ['年轻女性', 0], ['中年男性', 0]]\n",
            "[{'character': '旁白', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}, {'character': '年轻女性', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}, {'character': '中年男性', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}]\n",
            "{'旁白': {'character': '旁白', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}, '年轻女性': {'character': '年轻女性', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}, '中年男性': {'character': '中年男性', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}}\n",
            "seed=0 t=['在一个风和日丽的下午，小红帽准备去森林里看望她的奶奶。', '小红帽说', '在森林里，小红帽遇到了狡猾的大灰狼。', '大灰狼说', '小红帽回答'] c=旁白 s=5 r=[oral_2][laugh_0][break_4]\n",
            "speaker_type: seed\n",
            "Inferring audio for seed=2657:   0% 0/2 [00:00<?, ?steps/s]INFO:ChatTTS.core:All initialized.\n",
            "Inferring audio for seed=2657:   0% 0/2 [00:00<?, ?steps/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2103, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1650, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 870, in generate_script_audio\n",
            "    wavs = generate_audio_for_seed(chat, int(seed), texts, DEFAULT_BATCH_SIZE, speed,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/tts_model.py\", line 110, in generate_audio_for_seed\n",
            "    wavs = chat.infer(batch, params_infer_code=_params_infer_code, params_refine_text=params_refine_text,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 259, in infer\n",
            "    return next(res_gen)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 187, in _infer\n",
            "    text_tokens = refine_text(\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/infer/api.py\", line 97, in refine_text\n",
            "    text_token = models['tokenizer'](text, return_tensors='pt', add_special_tokens=False, padding=True).to(device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2868, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2956, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3149, in batch_encode_plus\n",
            "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
            "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2769, in _get_padding_truncation_strategies\n",
            "    if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id < 0):\n",
            "                                                           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n",
            "    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\n",
            "AttributeError: BertTokenizerFast has no attribute pad_token\n",
            "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
            "ChatCompletion(id='4edebecd-b2e7-4d84-a025-bf29cadd4fc9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='故事文本:  \\n\"在一个风雨交加的夜晚，花木兰手持长剑，独自一人站在悬崖边。她的目光坚定，仿佛在等待着什么。突然，周树人从黑暗中走出，手持折扇，面带微笑。花木兰冷冷地说道：“周树人，你终于来了。”周树人微微一笑，回应道：“花木兰，你果然是个不简单的女子。”\"\\n\\n转换后的JSON格式:  \\n```json\\n[\\n    {\"txt\": \"在一个风雨交加的夜晚，花木兰手持长剑，独自一人站在悬崖边。她的目光坚定，仿佛在等待着什么。\", \"character\": \"旁白\"},\\n    {\"txt\": \"突然，周树人从黑暗中走出，手持折扇，面带微笑。\", \"character\": \"旁白\"},\\n    {\"txt\": \"周树人，你终于来了。\", \"character\": \"花木兰\"},\\n    {\"txt\": \"花木兰，你果然是个不简单的女子。\", \"character\": \"周树人\"}\\n]\\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741608801, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_3a5770e1b4_prod0225', usage=CompletionUsage(completion_tokens=198, prompt_tokens=360, total_tokens=558, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=360))\n",
            "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
            "ChatCompletion(id='8e5df976-d16d-4464-bc98-acab99032095', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='以下是将《花木兰大战周树人》故事文本转换为适合舞台或屏幕的剧本格式，并以JSON格式输出的结果。故事文本经过分解，旁白和角色对话清晰，符合角色身份。\\n\\n```json\\n[\\n    {\\n        \"txt\": \"在一个风雨交加的夜晚，江湖上流传着一个传说：花木兰与周树人即将在华山之巅展开一场惊天动地的对决。\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"花木兰，一位英姿飒爽的女侠，手持长剑，目光如炬。她站在华山之巅，迎风而立。\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"周树人，一位文采斐然的书生，手持折扇，神情淡然。他缓步走上山巅，与花木兰对视。\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"花木兰冷冷地说道\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"周先生，今日一战，既分高下，也决生死！\",\\n        \"character\": \"花木兰\"\\n    },\\n    {\\n        \"txt\": \"周树人微微一笑，折扇轻摇，说道\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"花女侠，何必如此？江湖恩怨，不过是一场虚妄。不如我们坐下来，品茶论道，岂不快哉？\",\\n        \"character\": \"周树人\"\\n    },\\n    {\\n        \"txt\": \"花木兰冷哼一声，长剑出鞘，剑光如虹。\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"周先生，你若不敢应战，便认输吧！\",\\n        \"character\": \"花木兰\"\\n    },\\n    {\\n        \"txt\": \"周树人收起折扇，神色凝重。他缓缓从袖中取出一支毛笔，说道\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"既然如此，周某便以笔为剑，领教花女侠的高招！\",\\n        \"character\": \"周树人\"\\n    },\\n    {\\n        \"txt\": \"两人对峙片刻，忽然间，花木兰身形一闪，长剑直指周树人。周树人则以毛笔为剑，招式飘逸，与花木兰战在一处。\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"花木兰的剑法凌厉，招招致命；周树人的笔法灵动，以柔克刚。两人你来我往，战得难解难分。\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"花木兰突然收剑，冷冷地说道\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"周先生，你的笔法虽妙，却少了杀伐之气。这样的武功，如何能在江湖立足？\",\\n        \"character\": \"花木兰\"\\n    },\\n    {\\n        \"txt\": \"周树人微微一笑，说道\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"花女侠，武功并非只有杀伐。以文会武，以柔克刚，才是真正的境界。\",\\n        \"character\": \"周树人\"\\n    },\\n    {\\n        \"txt\": \"花木兰沉默片刻，忽然收起长剑，说道\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"周先生，今日一战，是我输了。你的境界，我望尘莫及。\",\\n        \"character\": \"花木兰\"\\n    },\\n    {\\n        \"txt\": \"周树人收起毛笔，拱手说道\",\\n        \"character\": \"旁白\"\\n    },\\n    {\\n        \"txt\": \"花女侠过谦了。江湖路远，愿我们后会有期。\",\\n        \"character\": \"周树人\"\\n    },\\n    {\\n        \"txt\": \"两人相视一笑，各自离去。华山之巅，风雨渐歇，只留下一段传奇。\",\\n        \"character\": \"旁白\"\\n    }\\n]\\n```\\n\\n### 说明：\\n1. **旁白**：用于描述场景、动作和背景信息，帮助观众理解故事发展。\\n2. **角色对话**：根据角色身份分配对话，花木兰为“花木兰”，周树人为“周树人”。\\n3. **JSON格式**：每个段落包含`txt`（文本内容）和`character`（角色身份），便于解析和使用。\\n\\n希望这份剧本能满足您的需求！', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741608828, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_3a5770e1b4_prod0225', usage=CompletionUsage(completion_tokens=942, prompt_tokens=364, total_tokens=1306, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=320), prompt_cache_hit_tokens=320, prompt_cache_miss_tokens=44))\n",
            "[['旁白', 0], ['花木兰', 0], ['周树人', 0]]\n",
            "[{'character': '旁白', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}, {'character': '年轻女性', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}, {'character': '中年男性', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}]\n",
            "[['旁白', 0], ['花木兰', 0], ['周树人', 0]]\n",
            "[['旁白', 0], ['花木兰', 0], ['周树人', 0]]\n",
            "[{'character': '旁白', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}, {'character': '花木兰', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}, {'character': '周树人', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}]\n",
            "{'旁白': {'character': '旁白', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}, '花木兰': {'character': '花木兰', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}, '周树人': {'character': '周树人', 'seed': 0, 'speed': 5, 'oral': 2, 'laugh': 0, 'break': 4}}\n",
            "seed=0 t=['在一个风雨交加的夜晚，江湖上流传着一个传说，花木兰与周树人即将在华山之巅展开一场惊天动地的对决。', '花木兰，一位英姿飒爽的女侠，手持长剑，目光如炬。她站在华山之巅，迎风而立。', '周树人，一位文采斐然的书生，手持折扇，神情淡然。他缓步走上山巅，与花木兰对视。', '花木兰冷冷地说道', '周树人微微一笑，折扇轻摇，说道'] c=旁白 s=5 r=[oral_2][laugh_0][break_4]\n",
            "speaker_type: seed\n",
            "Inferring audio for seed=5099:   0% 0/2 [00:00<?, ?steps/s]INFO:ChatTTS.core:All initialized.\n",
            "Inferring audio for seed=5099:   0% 0/2 [00:00<?, ?steps/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2103, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1650, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 870, in generate_script_audio\n",
            "    wavs = generate_audio_for_seed(chat, int(seed), texts, DEFAULT_BATCH_SIZE, speed,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/tts_model.py\", line 110, in generate_audio_for_seed\n",
            "    wavs = chat.infer(batch, params_infer_code=_params_infer_code, params_refine_text=params_refine_text,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 259, in infer\n",
            "    return next(res_gen)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 187, in _infer\n",
            "    text_tokens = refine_text(\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/infer/api.py\", line 97, in refine_text\n",
            "    text_token = models['tokenizer'](text, return_tensors='pt', add_special_tokens=False, padding=True).to(device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2868, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2956, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3149, in batch_encode_plus\n",
            "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
            "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2769, in _get_padding_truncation_strategies\n",
            "    if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id < 0):\n",
            "                                                           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n",
            "    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\n",
            "AttributeError: BertTokenizerFast has no attribute pad_token\n",
            "['四川美食确实以辣闻名，但也有不辣的选择。比如甜水面、赖汤圆、蛋烘糕、叶儿粑等，这些小吃口味温和，甜而不腻，也很受欢迎。', '我是一个充满活力的人，喜欢运动，喜欢旅行，喜欢尝试新鲜事物。我喜欢挑战自己，不断突破自己的极限，让自己变得更加强大。', '罗森宣布将于7月24日退市，在华门店超6000家']\n",
            "speaker_type: seed\n",
            "Inferring audio for seed=9378:   0% 0/3 [00:00<?, ?steps/s]INFO:ChatTTS.core:All initialized.\n",
            "Inferring audio for seed=9378:   0% 0/3 [00:00<?, ?steps/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2103, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1650, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 212, in audio_interface\n",
            "    seeds = generate_seeds(num_seeds, texts, progress.tqdm)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 139, in generate_seeds\n",
            "    filename = generate_audio_for_seed(chat, seed, texts, 1, 5, \"[oral_2][laugh_0][break_4]\", None, 0.3, 0.7, 20)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/tts_model.py\", line 110, in generate_audio_for_seed\n",
            "    wavs = chat.infer(batch, params_infer_code=_params_infer_code, params_refine_text=params_refine_text,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 259, in infer\n",
            "    return next(res_gen)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 187, in _infer\n",
            "    text_tokens = refine_text(\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/infer/api.py\", line 97, in refine_text\n",
            "    text_token = models['tokenizer'](text, return_tensors='pt', add_special_tokens=False, padding=True).to(device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2868, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2956, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3149, in batch_encode_plus\n",
            "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
            "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2769, in _get_padding_truncation_strategies\n",
            "    if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id < 0):\n",
            "                                                           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n",
            "    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\n",
            "AttributeError: BertTokenizerFast has no attribute pad_token\n",
            "['四川美食确实以辣闻名，但也有不辣的选择。']\n",
            "speaker_type: seed\n",
            "Inferring audio for seed=2464:   0% 0/1 [00:00<?, ?steps/s]INFO:ChatTTS.core:All initialized.\n",
            "Inferring audio for seed=2464:   0% 0/1 [00:00<?, ?steps/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2103, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1650, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 212, in audio_interface\n",
            "    seeds = generate_seeds(num_seeds, texts, progress.tqdm)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 139, in generate_seeds\n",
            "    filename = generate_audio_for_seed(chat, seed, texts, 1, 5, \"[oral_2][laugh_0][break_4]\", None, 0.3, 0.7, 20)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/tts_model.py\", line 110, in generate_audio_for_seed\n",
            "    wavs = chat.infer(batch, params_infer_code=_params_infer_code, params_refine_text=params_refine_text,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 259, in infer\n",
            "    return next(res_gen)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 187, in _infer\n",
            "    text_tokens = refine_text(\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/infer/api.py\", line 97, in refine_text\n",
            "    text_token = models['tokenizer'](text, return_tensors='pt', add_special_tokens=False, padding=True).to(device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2868, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2956, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3149, in batch_encode_plus\n",
            "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
            "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2769, in _get_padding_truncation_strategies\n",
            "    if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id < 0):\n",
            "                                                           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n",
            "    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\n",
            "AttributeError: BertTokenizerFast has no attribute pad_token\n",
            "k=3, audios=10\n",
            "['四川美食确实以辣闻名，但也有不辣的选择。']\n",
            "speaker_type: seed\n",
            "Inferring audio for seed=826:   0% 0/1 [00:00<?, ?steps/s]INFO:ChatTTS.core:All initialized.\n",
            "Inferring audio for seed=826:   0% 0/1 [00:00<?, ?steps/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2103, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1650, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 212, in audio_interface\n",
            "    seeds = generate_seeds(num_seeds, texts, progress.tqdm)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 139, in generate_seeds\n",
            "    filename = generate_audio_for_seed(chat, seed, texts, 1, 5, \"[oral_2][laugh_0][break_4]\", None, 0.3, 0.7, 20)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/tts_model.py\", line 110, in generate_audio_for_seed\n",
            "    wavs = chat.infer(batch, params_infer_code=_params_infer_code, params_refine_text=params_refine_text,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 259, in infer\n",
            "    return next(res_gen)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 187, in _infer\n",
            "    text_tokens = refine_text(\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/infer/api.py\", line 97, in refine_text\n",
            "    text_token = models['tokenizer'](text, return_tensors='pt', add_special_tokens=False, padding=True).to(device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2868, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2956, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3149, in batch_encode_plus\n",
            "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
            "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2769, in _get_padding_truncation_strategies\n",
            "    if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id < 0):\n",
            "                                                           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n",
            "    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\n",
            "AttributeError: BertTokenizerFast has no attribute pad_token\n"
          ]
        }
      ]
    }
  ]
}